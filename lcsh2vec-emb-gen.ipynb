{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import RDF, DC, SKOS\n",
    "from rdflib.namespace import Namespace\n",
    "from rdflib.term import URIRef\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Dense, GRU, Dropout, Reshape, Merge, Bidirectional\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GLOVE_FILE = 'glove.6B.300d.txt'\n",
    "LCSH_SUBJECTS_FILE = 'lcsh_subjects.csv'\n",
    "CONCEPTS_FILE = 'concepts.csv'\n",
    "CONCEPT_EMB_VIZ_FILE = 'lcsh_emb_viz.csv'\n",
    "MODEL_WEIGHTS_FILE = 'weights.h5'\n",
    "CONCEPT_EMBEDDINGS_FILE = 'lcsh_embeddings.txt'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 250000\n",
    "MAX_NB_CONCEPTS = 500000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "RNG_SEED_1 = 1446557\n",
    "RNG_SEED_2 = 1337603\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and clean up LCSH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! wget -q -O authoritiessubjects.nt.skos.gz http://id.loc.gov/static/data/authoritiessubjects.nt.skos.gz\n",
    "! gunzip -q -f authoritiessubjects.nt.skos.gz\n",
    "! sed -i '' 's/@EN \\./@en \\./g' authoritiessubjects.nt.skos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LCSH linked data into in-memory graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "print(\"Started\", datetime.datetime.now())\n",
    "g.parse(\"authoritiessubjects.nt.skos\", format=\"nt\")\n",
    "print(\"  Ended\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract concepts and labels from graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Started\", datetime.datetime.now())\n",
    "concept_labels = []\n",
    "for concept in g.subjects(RDF.type, SKOS.Concept):\n",
    "    if type(concept) == URIRef:\n",
    "        for label in g.objects(concept, SKOS.prefLabel):\n",
    "            concept_labels.append([ str(concept), str(label) ])\n",
    "        for label in g.objects(concept, SKOS.altLabel):\n",
    "            concept_labels.append([ str(concept), str(label) ])\n",
    "print(\"  Ended\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save concepts and labels to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(concept_labels, columns=['concept', 'description'])\n",
    "df.to_csv(LCSH_SUBJECTS_FILE, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load concepts into training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(LCSH_SUBJECTS_FILE, sep='\\t', usecols=['concept', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[df['description'].str.contains('Tobruk')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame({'concept': df['concept'].values,\n",
    "                       'description': df.sample(frac=1, random_state=RNG_SEED_1)['description'].values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['label'] = 1\n",
    "neg_df['label'] = 0\n",
    "training_data = pd.concat([df, neg_df]).sample(frac=1, random_state=RNG_SEED_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(['concept'])['description'].apply(lambda x: ', '.join(x))\n",
    "concepts = pd.DataFrame({'concept': grouped.index, 'description': grouped.values})\n",
    "\n",
    "print('Concepts: %d' % len(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concepts.to_csv(CONCEPTS_FILE, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concepts = pd.read_csv(CONCEPTS_FILE, sep='\\t', usecols=['concept', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concepts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build concept and description word indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concept_index= {}\n",
    "for row in concepts.iterrows():\n",
    "    concept_index[row[1]['concept']] = row[0]\n",
    "nb_concepts = min(MAX_NB_CONCEPTS, len(concept_index))\n",
    "    \n",
    "print(\"Concepts in index: %d\" % nb_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptions = training_data['description'].values\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(descriptions.tolist())\n",
    "desc_word_sequences = tokenizer.texts_to_sequences(descriptions.tolist())\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "\n",
    "print(\"Words in index: %d\" % len(nb_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
    "! unzip -q -o glove.6B.zip\n",
    "! rm -f glove.6B.zip glove.6B.50d.txt glove.6B.100d.txt glove.6B.200d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare word embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_data = np.array([ concept_index[c] for c in training_data['concept'].values ])\n",
    "d_data = pad_sequences(desc_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array([ [0, 1] if l == 0 else [1, 0] for l in training_data['label'].values ])\n",
    "\n",
    "print('Shape of concept tensor:', c_data.shape)\n",
    "print('Shape of description tensor:', d_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_concepts = min(MAX_NB_CONCEPTS, len(concept_index))\n",
    "\n",
    "P = Sequential()\n",
    "P.add(Embedding(nb_concepts + 1, EMBEDDING_DIM, input_length=1))\n",
    "P.add(Reshape((EMBEDDING_DIM,)))\n",
    "Q = Sequential()\n",
    "Q.add(Embedding(nb_words + 1, \n",
    "                EMBEDDING_DIM, \n",
    "                weights=[word_embedding_matrix], \n",
    "                input_length=MAX_SEQUENCE_LENGTH, \n",
    "                trainable=False))\n",
    "Q.add(Bidirectional(GRU(EMBEDDING_DIM, dropout_W=0.5, dropout_U=0.5), merge_mode='sum'))\n",
    "model = Sequential()\n",
    "model.add(Merge([P, Q], mode='concat'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(EMBEDDING_DIM*2, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_categorical_accuracy', save_best_only=True)]\n",
    "\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "\n",
    "history = model.fit([c_data, d_data], \n",
    "                    labels, \n",
    "                    nb_epoch=40, \n",
    "                    validation_split=VALIDATION_SPLIT, \n",
    "                    verbose=1, \n",
    "                    callbacks=callbacks)\n",
    "\n",
    "print(\"Training ended at\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                    'training': history.history['categorical_accuracy'],\n",
    "                    'validation': history.history['val_categorical_accuracy']})\n",
    "ax = acc.ix[:,:].plot(x='epoch', figsize={7,10}, grid=True)\n",
    "ax.set_ylabel(\"categorical accuracy\")\n",
    "ax.set_ylim([0.0,1.0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
    "                     'training': history.history['loss'],\n",
    "                     'validation': history.history['val_loss']})\n",
    "ax = loss.ix[:,:].plot(x='epoch', figsize={7,10}, grid=True)\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_ylim([0.0,2.0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and save to file concept embeddings from best model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_WEIGHTS_FILE)\n",
    "weights = P.layers[0].get_weights()[0]\n",
    "embeddings = pd.DataFrame(weights[1:])\n",
    "embeddings = pd.concat([concepts['concept'], embeddings], axis=1)\n",
    "\n",
    "embeddings.to_csv(CONCEPT_EMBEDDINGS_FILE, sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot and save to file t-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne2 = TSNE(n_components=2, perplexity=30, init='pca', n_iter=5000)\n",
    "fit = tsne2.fit_transform(weights)\n",
    "visualization = pd.DataFrame(fit[1:], columns=['x', 'y'])\n",
    "visualization.plot('x', 'y', kind='scatter', figsize={7,10}, grid=True);\n",
    "\n",
    "visualization.to_csv(CONCEPT_EMB_VIZ_FILE)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dlnotebook35]",
   "language": "python",
   "name": "conda-env-dlnotebook35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
